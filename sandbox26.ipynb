{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import mlflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Image Inpainting GAN1\")  # Create or select an experiment\n",
    "mlflow.tensorflow.autolog()  # Automatically logs TensorFlow metrics and parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_pairs(input_folder, gt_folder, img_size=(200, 200), batch_size=32, max_images=None):\n",
    "    \"\"\"\n",
    "    Load paired images for training (input with holes and ground truth), with an optional limit.\n",
    "    Args:\n",
    "        input_folder (str): Path to images with holes (e.g., HoledImages).\n",
    "        gt_folder (str): Path to original images (e.g., VegetableImages).\n",
    "        img_size (tuple): Size to resize images (default is 128x128).\n",
    "        batch_size (int): Number of images in a batch.\n",
    "        max_images (int, optional): Maximum number of images to include. Default is None (no limit).\n",
    "    Returns:\n",
    "        tf.data.Dataset: Dataset with paired images.\n",
    "    \"\"\"\n",
    "    def parse_pair(input_path, gt_path):\n",
    "        # Load and preprocess input image\n",
    "        input_img = tf.io.read_file(input_path)\n",
    "        input_img = tf.image.decode_jpeg(input_img, channels=3)\n",
    "        input_img = tf.image.resize(input_img, img_size) / 255.0  # Normalize\n",
    "        \n",
    "        # Load and preprocess ground truth image\n",
    "        gt_img = tf.io.read_file(gt_path)\n",
    "        gt_img = tf.image.decode_jpeg(gt_img, channels=3)\n",
    "        gt_img = tf.image.resize(gt_img, img_size) / 255.0  # Normalize\n",
    "        \n",
    "        return input_img, gt_img\n",
    "\n",
    "    # Get paired file paths\n",
    "    input_paths = sorted(glob(f\"{input_folder}/**/*.jpg\", recursive=True))\n",
    "    gt_paths = sorted(glob(f\"{gt_folder}/**/*.jpg\", recursive=True))\n",
    "    \n",
    "    # Limit the number of images\n",
    "    if max_images is not None:\n",
    "        input_paths = input_paths[:max_images]\n",
    "        gt_paths = gt_paths[:max_images]\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_paths, gt_paths))\n",
    "    dataset = dataset.map(lambda x, y: parse_pair(x, y))\n",
    "    dataset = dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_image_pairs(\n",
    "    \"./Data/HoledImages/train\",\n",
    "    \"./Data/VegetableImages/train\",\n",
    "    max_images=200  # Limit to 1000 images\n",
    ")\n",
    "val_data = load_image_pairs(\n",
    "    \"./Data/HoledImages/validation\",\n",
    "    \"./Data/VegetableImages/validation\",\n",
    "    max_images=100\n",
    ")\n",
    "test_data = load_image_pairs(\n",
    "    \"./Data/HoledImages/test\",\n",
    "    \"./Data/VegetableImages/test\",\n",
    "    max_images=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(input_images):\n",
    "    \"\"\"\n",
    "    Create a mask for black square regions in the input images.\n",
    "    Args:\n",
    "        input_images (tf.Tensor): Batch of input images with holes.\n",
    "    Returns:\n",
    "        tf.Tensor: Binary mask with 1s for black square regions and 0s elsewhere.\n",
    "    \"\"\"\n",
    "    # Black squares are assumed to have pixel values close to 0\n",
    "    mask = tf.cast(tf.reduce_all(input_images < 0.1, axis=-1, keepdims=True), tf.float32)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Generator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Generator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,040</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,459</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d (\u001b[38;5;33mUpSampling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │       \u001b[38;5;34m295,040\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d_1 (\u001b[38;5;33mUpSampling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │         \u001b[38;5;34m3,459\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,259,395</span> (4.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,259,395\u001b[0m (4.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,259,395</span> (4.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,259,395\u001b[0m (4.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_generator(img_size=(200, 200, 3)):\n",
    "    \"\"\"\n",
    "    Generator model: U-Net-style architecture.\n",
    "    Args:\n",
    "        img_size (tuple): Input image size (default is (200, 200, 3)).\n",
    "    Returns:\n",
    "        keras.Model: Generator model.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=img_size)\n",
    "\n",
    "    # Encoder\n",
    "    x1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x1 = layers.MaxPooling2D((2, 2))(x1)\n",
    "\n",
    "    x2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x1)\n",
    "    x2 = layers.MaxPooling2D((2, 2))(x2)\n",
    "\n",
    "    # Bottleneck\n",
    "    x3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x2)\n",
    "    x3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x3)\n",
    "\n",
    "    # Decoder\n",
    "    x4 = layers.UpSampling2D((2, 2))(x3)\n",
    "    x4 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x4)\n",
    "\n",
    "    x5 = layers.UpSampling2D((2, 2))(x4)\n",
    "    outputs = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x5)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs, name=\"Generator\")\n",
    "\n",
    "generator = build_generator()\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Discriminator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Discriminator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160000</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">160,001</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m131,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m524,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160000\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │       \u001b[38;5;34m160,001\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">818,881</span> (3.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m818,881\u001b[0m (3.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">818,881</span> (3.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m818,881\u001b[0m (3.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_discriminator(img_size=(200, 200, 3)):\n",
    "    \"\"\"\n",
    "    Discriminator model: Patch-based classification.\n",
    "    Args:\n",
    "        img_size (tuple): Input image size (default is (200, 200, 3)).\n",
    "    Returns:\n",
    "        keras.Model: Discriminator model.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=img_size)\n",
    "\n",
    "    x = layers.Conv2D(64, (4, 4), strides=2, activation='relu', padding='same')(inputs)\n",
    "    x = layers.Conv2D(128, (4, 4), strides=2, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(256, (4, 4), strides=2, activation='relu', padding='same')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs, name=\"Discriminator\")\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"GAN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"GAN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Generator (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,259,395</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Discriminator (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">818,881</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Generator (\u001b[38;5;33mFunctional\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │     \u001b[38;5;34m1,259,395\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Discriminator (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │       \u001b[38;5;34m818,881\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,078,276</span> (7.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,078,276\u001b[0m (7.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,078,276</span> (7.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,078,276\u001b[0m (7.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_gan(generator, discriminator):\n",
    "    \"\"\"\n",
    "    Combines generator and discriminator into a GAN model.\n",
    "    Args:\n",
    "        generator (keras.Model): Generator model.\n",
    "        discriminator (keras.Model): Discriminator model.\n",
    "    Returns:\n",
    "        keras.Model: Combined GAN model.\n",
    "    \"\"\"\n",
    "    # discriminator.trainable = False  # Freeze discriminator for GAN training\n",
    "\n",
    "    gan_input = layers.Input(shape=(200, 200, 3))\n",
    "    generated_image = generator(gan_input)\n",
    "    gan_output = discriminator(generated_image)\n",
    "\n",
    "    return tf.keras.Model(gan_input, gan_output, name=\"GAN\")\n",
    "\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GAN loss function\n",
    "def gan_loss(real_images, generated_images, fake_output):\n",
    "    # Adversarial loss (encourages realistic generation)\n",
    "    adversarial_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(fake_output), fake_output)\n",
    "    # Reconstruction loss (encourages similarity to ground truth)\n",
    "    reconstruction_loss = tf.keras.losses.MeanSquaredError()(real_images, generated_images)\n",
    "    return adversarial_loss + reconstruction_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the discriminator\n",
    "discriminator.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=disc_optimizer,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Compile the GAN\n",
    "gan.compile(\n",
    "    loss=gan_loss,\n",
    "    optimizer=gen_optimizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator trainable variables: 8\n",
      "Generator trainable variables: 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Discriminator trainable variables: {len(discriminator.trainable_variables)}\")\n",
    "print(f\"Generator trainable variables: {len(generator.trainable_variables)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(generator, discriminator, input_images, gt_images, gen_optimizer, disc_optimizer):\n",
    "    \"\"\"\n",
    "    Perform one training step for both generator and discriminator with masking for inpainting.\n",
    "    \"\"\"\n",
    "    # Create a mask for black square regions\n",
    "    mask = create_mask(input_images)\n",
    "\n",
    "    # Ensure discriminator is trainable\n",
    "    discriminator.trainable = True\n",
    "\n",
    "    # Train discriminator\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        # Generate inpainted images\n",
    "        generated_images = generator(input_images, training=True)\n",
    "\n",
    "        # Merge inpainted regions with unmasked regions\n",
    "        inpainted_images = mask * generated_images + (1 - mask) * input_images\n",
    "\n",
    "        real_output = discriminator(gt_images, training=True)\n",
    "        fake_output = discriminator(inpainted_images, training=True)\n",
    "\n",
    "        disc_loss = (\n",
    "            tf.keras.losses.BinaryCrossentropy()(tf.ones_like(real_output), real_output) +\n",
    "            tf.keras.losses.BinaryCrossentropy()(tf.zeros_like(fake_output), fake_output)\n",
    "        )\n",
    "\n",
    "    gradients_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    disc_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))\n",
    "\n",
    "    # Train generator\n",
    "    discriminator.trainable = False  # Freeze discriminator for GAN training\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        # Generate inpainted images\n",
    "        generated_images = generator(input_images, training=True)\n",
    "\n",
    "        # Merge inpainted regions with unmasked regions\n",
    "        inpainted_images = mask * generated_images + (1 - mask) * input_images\n",
    "\n",
    "        fake_output = discriminator(inpainted_images, training=False)\n",
    "\n",
    "        # Calculate the generator loss\n",
    "        gen_loss = (\n",
    "            tf.keras.losses.BinaryCrossentropy()(tf.ones_like(fake_output), fake_output) +  # Adversarial loss\n",
    "            tf.keras.losses.MeanSquaredError()(mask * gt_images, mask * generated_images)  # Reconstruction loss\n",
    "        )\n",
    "\n",
    "    gradients_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gen_optimizer.apply_gradients(zip(gradients_gen, generator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_log_images(generator, input_images, gt_images, log_path, epoch=None, step=None, prefix=\"test_results\"):\n",
    "    \"\"\"\n",
    "    Visualize sample inputs, inpainted outputs, and ground truth; save to MLflow.\n",
    "    \"\"\"\n",
    "    generated_images = generator(input_images, training=False)\n",
    "    mask = create_mask(input_images)\n",
    "    inpainted_images = mask * generated_images + (1 - mask) * input_images  # Merge inpainted regions\n",
    "\n",
    "    num_samples = min(5, input_images.shape[0])\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, num_samples * 3))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        axes[i, 0].imshow(input_images[i].numpy())\n",
    "        axes[i, 0].set_title(\"Input (With Holes)\")\n",
    "        axes[i, 1].imshow(inpainted_images[i].numpy())\n",
    "        axes[i, 1].set_title(\"Inpainted (Generated)\")\n",
    "        axes[i, 2].imshow(gt_images[i].numpy())\n",
    "        axes[i, 2].set_title(\"Ground Truth\")\n",
    "        for ax in axes[i]:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    file_name = f\"{prefix}_epoch_{int(epoch)+1}_step_{int(step)+1}.png\" if epoch is not None and step is not None else f\"{prefix}.png\"\n",
    "    output_path = os.path.join(log_path, file_name)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(output_path, artifact_path=\"visualizations\")\n",
    "\n",
    "    print(f\"Visualization saved and logged: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved and logged: ./training_logs\\test_results_epoch_1_step_6.png\n",
      "Epoch 1/20 | Gen Loss: 0.7467, Disc Loss: 1.3804\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_2_step_6.png\n",
      "Epoch 2/20 | Gen Loss: 0.7212, Disc Loss: 1.3804\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_3_step_6.png\n",
      "Epoch 3/20 | Gen Loss: 0.6983, Disc Loss: 1.3784\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_4_step_6.png\n",
      "Epoch 4/20 | Gen Loss: 0.7202, Disc Loss: 1.3573\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_5_step_6.png\n",
      "Epoch 5/20 | Gen Loss: 0.7385, Disc Loss: 1.4084\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_6_step_6.png\n",
      "Epoch 6/20 | Gen Loss: 0.7406, Disc Loss: 1.3303\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_7_step_6.png\n",
      "Epoch 7/20 | Gen Loss: 0.7502, Disc Loss: 1.3378\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_8_step_6.png\n",
      "Epoch 8/20 | Gen Loss: 0.8028, Disc Loss: 1.3223\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_9_step_6.png\n",
      "Epoch 9/20 | Gen Loss: 0.7692, Disc Loss: 1.3195\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_10_step_6.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Gen Loss: 0.8196, Disc Loss: 1.3166\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_11_step_6.png\n",
      "Epoch 11/20 | Gen Loss: 0.7950, Disc Loss: 1.2780\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_12_step_6.png\n",
      "Epoch 12/20 | Gen Loss: 0.8663, Disc Loss: 1.3182\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_13_step_6.png\n",
      "Epoch 13/20 | Gen Loss: 0.8521, Disc Loss: 1.2597\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_14_step_6.png\n",
      "Epoch 14/20 | Gen Loss: 0.8349, Disc Loss: 1.2890\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_15_step_6.png\n",
      "Epoch 15/20 | Gen Loss: 0.8279, Disc Loss: 1.3625\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_16_step_6.png\n",
      "Epoch 16/20 | Gen Loss: 0.7637, Disc Loss: 1.3057\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_17_step_6.png\n",
      "Epoch 17/20 | Gen Loss: 0.8373, Disc Loss: 1.2874\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_18_step_6.png\n",
      "Epoch 18/20 | Gen Loss: 0.8802, Disc Loss: 1.2564\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_19_step_6.png\n",
      "Epoch 19/20 | Gen Loss: 0.7052, Disc Loss: 1.4245\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_20_step_6.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 | Gen Loss: 0.7898, Disc Loss: 1.3200\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "log_path = \"./training_logs\"\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"epochs\", EPOCHS)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"image_size\", (200, 200))\n",
    "    mlflow.log_param(\"batch_size\", 32)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        step = 0\n",
    "        gen_loss_epoch = 0\n",
    "        disc_loss_epoch = 0\n",
    "        \n",
    "        for input_images, gt_images in train_data:  # Iterate over training batches\n",
    "            gen_loss, disc_loss = train_step(generator, discriminator, input_images, gt_images, gen_optimizer, disc_optimizer)\n",
    "            \n",
    "            gen_loss_epoch += gen_loss.numpy()\n",
    "            disc_loss_epoch += disc_loss.numpy()\n",
    "            step += 1\n",
    "\n",
    "            # Visualize and log images every 100 steps\n",
    "            if step % 5 == 0:\n",
    "                visualize_and_log_images(generator, input_images, gt_images, log_path, epoch, step)\n",
    "\n",
    "        # Average losses\n",
    "        gen_loss_epoch /= step\n",
    "        disc_loss_epoch /= step\n",
    "\n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metric(\"generator_loss\", gen_loss_epoch, step=epoch)\n",
    "        mlflow.log_metric(\"discriminator_loss\", disc_loss_epoch, step=epoch)\n",
    "\n",
    "        # Save models every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            gen_model_path = f\"generator_epoch_{epoch+1}.h5\"\n",
    "            disc_model_path = f\"discriminator_epoch_{epoch+1}.h5\"\n",
    "            generator.save(gen_model_path)\n",
    "            discriminator.save(disc_model_path)\n",
    "            mlflow.log_artifact(gen_model_path, artifact_path=\"models\")\n",
    "            mlflow.log_artifact(disc_model_path, artifact_path=\"models\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Gen Loss: {gen_loss_epoch:.4f}, Disc Loss: {disc_loss_epoch:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved and logged: ./test_visualizations\\test_batch_1.png\n",
      "Visualization saved and logged: ./test_visualizations\\test_batch_2.png\n",
      "Visualization saved and logged: ./test_visualizations\\test_batch_3.png\n",
      "Visualization saved and logged: ./test_visualizations\\test_batch_4.png\n",
      "Test Gen Loss: 0.7335, Test Disc Loss: 1.6580\n"
     ]
    }
   ],
   "source": [
    "test_gen_loss = 0\n",
    "test_disc_loss = 0\n",
    "test_steps = 0\n",
    "\n",
    "log_path = \"./test_visualizations\"\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "for i, (test_input_images, test_gt_images) in enumerate(test_data):\n",
    "    generated_images = generator(test_input_images, training=False)\n",
    "    fake_output = discriminator(generated_images, training=False)\n",
    "    real_output = discriminator(test_gt_images, training=False)\n",
    "\n",
    "    test_gen_loss += gan_loss(test_gt_images, generated_images, fake_output).numpy()\n",
    "    test_disc_loss += (\n",
    "        tf.keras.losses.BinaryCrossentropy()(tf.ones_like(real_output), real_output).numpy() +\n",
    "        tf.keras.losses.BinaryCrossentropy()(tf.zeros_like(fake_output), fake_output).numpy()\n",
    "    )\n",
    "    test_steps += 1\n",
    "\n",
    "    # Visualize and log test images (limit to 5 batches for clarity)\n",
    "    if i < 5:\n",
    "        visualize_and_log_images(\n",
    "            generator, test_input_images, test_gt_images, log_path, prefix=f\"test_batch_{i+1}\"\n",
    "        )\n",
    "\n",
    "test_gen_loss /= test_steps\n",
    "test_disc_loss /= test_steps\n",
    "\n",
    "print(f\"Test Gen Loss: {test_gen_loss:.4f}, Test Disc Loss: {test_disc_loss:.4f}\")\n",
    "mlflow.log_metric(\"test_generator_loss\", test_gen_loss)\n",
    "mlflow.log_metric(\"test_discriminator_loss\", test_disc_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
