{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import mlflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Image Inpainting GAN1\")  # Create or select an experiment\n",
    "mlflow.tensorflow.autolog()  # Automatically logs TensorFlow metrics and parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_images(input_image, gt_image):\n",
    "    # Apply random flipping and rotations\n",
    "    input_image = tf.image.random_flip_left_right(input_image)\n",
    "    input_image = tf.image.random_flip_up_down(input_image)\n",
    "    input_image = tf.image.rot90(input_image, k=random.randint(0, 3))\n",
    "    \n",
    "    gt_image = tf.image.random_flip_left_right(gt_image)\n",
    "    gt_image = tf.image.random_flip_up_down(gt_image)\n",
    "    gt_image = tf.image.rot90(gt_image, k=random.randint(0, 3))\n",
    "\n",
    "    return input_image, gt_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_pairs(input_folder, gt_folder, img_size=(200, 200), batch_size=32, max_images=None):\n",
    "    \"\"\"\n",
    "    Load paired images for training (input with holes and ground truth), with an optional limit.\n",
    "    Args:\n",
    "        input_folder (str): Path to images with holes (e.g., HoledImages).\n",
    "        gt_folder (str): Path to original images (e.g., VegetableImages).\n",
    "        img_size (tuple): Size to resize images (default is 128x128).\n",
    "        batch_size (int): Number of images in a batch.\n",
    "        max_images (int, optional): Maximum number of images to include. Default is None (no limit).\n",
    "    Returns:\n",
    "        tf.data.Dataset: Dataset with paired images.\n",
    "    \"\"\"\n",
    "    def parse_pair(input_path, gt_path):\n",
    "        # Load and preprocess input image\n",
    "        input_img = tf.io.read_file(input_path)\n",
    "        input_img = tf.image.decode_jpeg(input_img, channels=3)\n",
    "        input_img = tf.image.resize(input_img, img_size) / 255.0  # Normalize\n",
    "        \n",
    "        # Load and preprocess ground truth image\n",
    "        gt_img = tf.io.read_file(gt_path)\n",
    "        gt_img = tf.image.decode_jpeg(gt_img, channels=3)\n",
    "        gt_img = tf.image.resize(gt_img, img_size) / 255.0  # Normalize\n",
    "        \n",
    "        return input_img, gt_img\n",
    "\n",
    "    # Get paired file paths\n",
    "    input_paths = sorted(glob(f\"{input_folder}/**/*.jpg\", recursive=True))\n",
    "    gt_paths = sorted(glob(f\"{gt_folder}/**/*.jpg\", recursive=True))\n",
    "    \n",
    "    # Limit the number of images\n",
    "    if max_images is not None:\n",
    "        input_paths = input_paths[:max_images]\n",
    "        gt_paths = gt_paths[:max_images]\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_paths, gt_paths))\n",
    "    dataset = dataset.map(lambda x, y: parse_pair(x, y))\n",
    "    dataset = dataset.map(augment_images)\n",
    "    dataset = dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_image_pairs(\n",
    "    \"./Data/HoledImages/train\",\n",
    "    \"./Data/VegetableImages/train\",\n",
    "    max_images=200  # Limit to 1000 images\n",
    ")\n",
    "val_data = load_image_pairs(\n",
    "    \"./Data/HoledImages/validation\",\n",
    "    \"./Data/VegetableImages/validation\",\n",
    "    max_images=100\n",
    ")\n",
    "test_data = load_image_pairs(\n",
    "    \"./Data/HoledImages/test\",\n",
    "    \"./Data/VegetableImages/test\",\n",
    "    max_images=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(input_images):\n",
    "    \"\"\"\n",
    "    Create a mask for black square regions in the input images.\n",
    "    Args:\n",
    "        input_images (tf.Tensor): Batch of input images with holes.\n",
    "    Returns:\n",
    "        tf.Tensor: Binary mask with 1s for black square regions and 0s elsewhere.\n",
    "    \"\"\"\n",
    "    # Black squares are assumed to have pixel values close to 0\n",
    "    mask = tf.cast(tf.reduce_all(input_images == 0, axis=-1, keepdims=True), tf.float32)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_mask(input_images, max_percentage=10):\n",
    "#     \"\"\"\n",
    "#     Create a mask with a single black square in the center, matching the black square added during preprocessing.\n",
    "\n",
    "#     Args:\n",
    "#         input_images (tf.Tensor): Batch of input images.\n",
    "#         max_percentage (float): Maximum percentage of the image area covered by the square.\n",
    "\n",
    "#     Returns:\n",
    "#         tf.Tensor: Binary mask with 0s for the black square and 1s elsewhere.\n",
    "#     \"\"\"\n",
    "#     batch_size, img_height, img_width, _ = input_images.shape\n",
    "#     mask = np.ones((batch_size, img_height, img_width, 1), dtype=np.float32)\n",
    "\n",
    "#     # Calculate the square size\n",
    "#     max_area = (img_height * img_width) * (max_percentage / 100)\n",
    "#     side_length = int(max_area ** 0.5)\n",
    "\n",
    "#     # Center position of the square\n",
    "#     y0 = (img_height - side_length) // 2\n",
    "#     x0 = (img_width - side_length) // 2\n",
    "#     y1 = y0 + side_length\n",
    "#     x1 = x0 + side_length\n",
    "\n",
    "#     # Apply the black square mask\n",
    "#     for i in range(batch_size):\n",
    "#         mask[i, y0:y1, x0:x1, :] = 0\n",
    "\n",
    "#     return tf.convert_to_tensor(mask, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_black_square_and_mask(image_path, mask):\n",
    "#     \"\"\"\n",
    "#     Visualize the image with a black square and the corresponding mask.\n",
    "\n",
    "#     Args:\n",
    "#         image_path (str): Path to the input image with a black square.\n",
    "#         mask (tf.Tensor): Generated mask tensor.\n",
    "#     \"\"\"\n",
    "#     # Decode and preprocess the image\n",
    "#     image = tf.image.decode_jpeg(tf.io.read_file(image_path))\n",
    "#     image = tf.image.resize(image, (200, 200))  # Resize to match the mask size\n",
    "#     image = tf.cast(image, tf.float32) / 255.0  # Cast to float32 and normalize\n",
    "\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "\n",
    "#     # Display the image\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.imshow(image)\n",
    "#     plt.title(\"Image with Black Square\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     # Display the mask\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.imshow(mask[0, :, :, 0], cmap=\"gray\")\n",
    "#     plt.title(\"Mask\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # Example\n",
    "# image_path = \"./Data/HoledImages/train/Bean/0026.jpg\"  # Replace with a valid image path\n",
    "# sample_image = tf.image.decode_jpeg(tf.io.read_file(image_path))\n",
    "# sample_image = tf.image.resize(sample_image, (200, 200))\n",
    "# sample_image = tf.cast(sample_image, tf.float32) / 255.0  # Cast to float32 and normalize\n",
    "# sample_image = tf.expand_dims(sample_image, axis=0)  # Add batch dimension\n",
    "\n",
    "# # Create a mask for the sample image\n",
    "# mask = create_mask(sample_image, max_percentage=10)\n",
    "# visualize_black_square_and_mask(image_path, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_generator(img_size=(200, 200, 3)):\n",
    "#     inputs = layers.Input(shape=img_size)\n",
    "\n",
    "#     # Encoder\n",
    "#     x1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "#     x1 = layers.BatchNormalization()(x1)\n",
    "#     x1 = layers.MaxPooling2D((2, 2))(x1)  # Output: (100, 100, 64)\n",
    "\n",
    "#     x2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x1)\n",
    "#     x2 = layers.BatchNormalization()(x2)\n",
    "#     x2 = layers.MaxPooling2D((2, 2))(x2)  # Output: (50, 50, 128)\n",
    "\n",
    "#     # Bottleneck\n",
    "#     x3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x2)\n",
    "#     x3 = layers.BatchNormalization()(x3)\n",
    "#     x3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x3)\n",
    "#     x3 = layers.Dropout(0.3)(x3)  # Output: (50, 50, 256)\n",
    "\n",
    "#     # Decoder with skip connections\n",
    "#     x4 = layers.UpSampling2D((2, 2))(x3)  # Output: (100, 100, 256)\n",
    "#     x4 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x4)\n",
    "#     x4 = layers.BatchNormalization()(x4)\n",
    "\n",
    "#     # Align spatial dimensions of `x2_resized` to match `x4`\n",
    "#     x2_resized = layers.Conv2D(128, (1, 1), activation='relu', padding='same')(x2)\n",
    "#     x2_resized = layers.UpSampling2D((2, 2))(x2_resized)  # Now (100, 100, 128)\n",
    "#     x4 = layers.Add()([x4, x2_resized])  # Now compatible for addition\n",
    "\n",
    "#     x5 = layers.UpSampling2D((2, 2))(x4)  # Output: (200, 200, 128)\n",
    "#     x5 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x5)\n",
    "#     x5 = layers.BatchNormalization()(x5)\n",
    "\n",
    "#     # Align spatial dimensions of `x1_resized` to match `x5`\n",
    "#     x1_resized = layers.Conv2D(64, (1, 1), activation='relu', padding='same')(x1)  # (100, 100, 64)\n",
    "#     x1_resized = layers.UpSampling2D((2, 2))(x1_resized)  # Now (200, 200, 64)\n",
    "#     x5 = layers.Add()([x5, x1_resized])  # Now compatible for addition\n",
    "\n",
    "#     outputs = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x5)  # Output: (200, 200, 3)\n",
    "\n",
    "#     return tf.keras.Model(inputs, outputs, name=\"Generator\")\n",
    "\n",
    "\n",
    "# generator = build_generator()\n",
    "# generator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Generator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Generator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,040</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,459</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d (\u001b[38;5;33mUpSampling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │       \u001b[38;5;34m295,040\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d_1 (\u001b[38;5;33mUpSampling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │         \u001b[38;5;34m3,459\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,259,395</span> (4.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,259,395\u001b[0m (4.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,259,395</span> (4.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,259,395\u001b[0m (4.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_generator(img_size=(200, 200, 3)):\n",
    "    \"\"\"\n",
    "    Generator model: U-Net-style architecture.\n",
    "    Args:\n",
    "        img_size (tuple): Input image size (default is (200, 200, 3)).\n",
    "    Returns:\n",
    "        keras.Model: Generator model.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=img_size)\n",
    "\n",
    "    # Encoder\n",
    "    x1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x1 = layers.MaxPooling2D((2, 2))(x1)\n",
    "\n",
    "    x2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x1)\n",
    "    x2 = layers.MaxPooling2D((2, 2))(x2)\n",
    "\n",
    "    # Bottleneck\n",
    "    x3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x2)\n",
    "    x3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x3)\n",
    "\n",
    "    # Decoder\n",
    "    x4 = layers.UpSampling2D((2, 2))(x3)\n",
    "    x4 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x4)\n",
    "\n",
    "    x5 = layers.UpSampling2D((2, 2))(x4)\n",
    "    outputs = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x5)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs, name=\"Generator\")\n",
    "\n",
    "generator = build_generator()\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Discriminator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Discriminator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160000</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">160,001</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m131,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m524,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160000\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │       \u001b[38;5;34m160,001\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">818,881</span> (3.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m818,881\u001b[0m (3.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">818,881</span> (3.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m818,881\u001b[0m (3.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_discriminator(img_size=(200, 200, 3)):\n",
    "    \"\"\"\n",
    "    Discriminator model: Patch-based classification.\n",
    "    Args:\n",
    "        img_size (tuple): Input image size (default is (200, 200, 3)).\n",
    "    Returns:\n",
    "        keras.Model: Discriminator model.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=img_size)\n",
    "\n",
    "    x = layers.Conv2D(64, (4, 4), strides=2, activation='relu', padding='same')(inputs)\n",
    "    x = layers.Conv2D(128, (4, 4), strides=2, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(256, (4, 4), strides=2, activation='relu', padding='same')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs, name=\"Discriminator\")\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"GAN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"GAN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Generator (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,259,395</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Discriminator (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">818,881</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Generator (\u001b[38;5;33mFunctional\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │     \u001b[38;5;34m1,259,395\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Discriminator (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │       \u001b[38;5;34m818,881\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,078,276</span> (7.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,078,276\u001b[0m (7.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,078,276</span> (7.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,078,276\u001b[0m (7.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_gan(generator, discriminator):\n",
    "    \"\"\"\n",
    "    Combines generator and discriminator into a GAN model.\n",
    "    Args:\n",
    "        generator (keras.Model): Generator model.\n",
    "        discriminator (keras.Model): Discriminator model.\n",
    "    Returns:\n",
    "        keras.Model: Combined GAN model.\n",
    "    \"\"\"\n",
    "    # discriminator.trainable = False  # Freeze discriminator for GAN training\n",
    "\n",
    "    gan_input = layers.Input(shape=(200, 200, 3))\n",
    "    generated_image = generator(gan_input)\n",
    "    gan_output = discriminator(generated_image)\n",
    "\n",
    "    return tf.keras.Model(gan_input, gan_output, name=\"GAN\")\n",
    "\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GAN loss function\n",
    "def gan_loss(real_images, generated_images, fake_output):\n",
    "    # Adversarial loss (encourages realistic generation)\n",
    "    adversarial_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(fake_output), fake_output)\n",
    "    # Reconstruction loss (encourages similarity to ground truth)\n",
    "    reconstruction_loss = tf.keras.losses.MeanSquaredError()(real_images, generated_images)\n",
    "    return adversarial_loss + reconstruction_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add learning rate schedulers\n",
    "lr_schedule_gen = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4, decay_steps=10000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "lr_schedule_disc = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4, decay_steps=10000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# Use the schedulers in optimizers\n",
    "gen_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule_gen, beta_1=0.5)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule_disc, beta_1=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the discriminator\n",
    "discriminator.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=disc_optimizer,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Compile the GAN\n",
    "gan.compile(\n",
    "    loss=gan_loss,\n",
    "    optimizer=gen_optimizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator trainable variables: 8\n",
      "Generator trainable variables: 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Discriminator trainable variables: {len(discriminator.trainable_variables)}\")\n",
    "print(f\"Generator trainable variables: {len(generator.trainable_variables)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(generator, discriminator, input_images, gt_images, gen_optimizer, disc_optimizer):\n",
    "    \"\"\"\n",
    "    Perform one training step for both generator and discriminator with masking for inpainting.\n",
    "    \"\"\"\n",
    "    # Create a mask for black square regions\n",
    "    mask = create_mask(input_images)\n",
    "\n",
    "    # Ensure discriminator is trainable\n",
    "    discriminator.trainable = True\n",
    "\n",
    "    # Train discriminator\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        # Generate inpainted images\n",
    "        generated_images = generator(input_images, training=True)\n",
    "\n",
    "        # Merge inpainted regions with unmasked regions\n",
    "        inpainted_images = mask * generated_images + (1 - mask) * input_images\n",
    "\n",
    "        real_output = discriminator(gt_images, training=True)\n",
    "        fake_output = discriminator(inpainted_images, training=True)\n",
    "\n",
    "        disc_loss = (\n",
    "            tf.keras.losses.BinaryCrossentropy()(tf.ones_like(real_output), real_output) +\n",
    "            tf.keras.losses.BinaryCrossentropy()(tf.zeros_like(fake_output), fake_output)\n",
    "        )\n",
    "\n",
    "    gradients_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    disc_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))\n",
    "\n",
    "    # Train generator\n",
    "    discriminator.trainable = False  # Freeze discriminator for GAN training\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        # Generate inpainted images\n",
    "        generated_images = generator(input_images, training=True)\n",
    "\n",
    "        # Merge inpainted regions with unmasked regions\n",
    "        inpainted_images = mask * generated_images + (1 - mask) * input_images\n",
    "\n",
    "        fake_output = discriminator(inpainted_images, training=False)\n",
    "\n",
    "        # Calculate the generator loss\n",
    "        gen_loss = (\n",
    "            tf.keras.losses.BinaryCrossentropy()(tf.ones_like(fake_output), fake_output) +  # Adversarial loss\n",
    "            tf.keras.losses.MeanSquaredError()(mask * gt_images, mask * generated_images)  # Reconstruction loss\n",
    "        )\n",
    "\n",
    "    gradients_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gen_optimizer.apply_gradients(zip(gradients_gen, generator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_log_images(generator, input_images, gt_images, log_path, epoch=None, step=None, prefix=\"test_results\"):\n",
    "    \"\"\"\n",
    "    Visualize sample inputs, inpainted outputs, and ground truth; save to MLflow.\n",
    "    \"\"\"\n",
    "    generated_images = generator(input_images, training=False)\n",
    "    mask = create_mask(input_images)\n",
    "    inpainted_images = mask * generated_images + (1 - mask) * input_images  # Merge inpainted regions\n",
    "\n",
    "    num_samples = min(5, input_images.shape[0])\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, num_samples * 3))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        axes[i, 0].imshow(input_images[i].numpy())\n",
    "        axes[i, 0].set_title(\"Input (With Holes)\")\n",
    "        axes[i, 1].imshow(inpainted_images[i].numpy())\n",
    "        axes[i, 1].set_title(\"Inpainted (Generated)\")\n",
    "        axes[i, 2].imshow(gt_images[i].numpy())\n",
    "        axes[i, 2].set_title(\"Ground Truth\")\n",
    "        for ax in axes[i]:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    file_name = f\"{prefix}_epoch_{int(epoch)+1}_step_{int(step)+1}.png\" if epoch is not None and step is not None else f\"{prefix}.png\"\n",
    "    output_path = os.path.join(log_path, file_name)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(output_path, artifact_path=\"visualizations\")\n",
    "\n",
    "    print(f\"Visualization saved and logged: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved and logged: ./training_logs\\test_results_epoch_1_step_6.png\n",
      "Epoch 1/20 | Gen Loss: 0.7453, Disc Loss: 1.3314\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_2_step_6.png\n",
      "Epoch 2/20 | Gen Loss: 0.9212, Disc Loss: 1.1567\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_3_step_6.png\n",
      "Epoch 3/20 | Gen Loss: 1.2391, Disc Loss: 1.0963\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_4_step_6.png\n",
      "Epoch 4/20 | Gen Loss: 1.3911, Disc Loss: 0.6953\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_5_step_6.png\n",
      "Epoch 5/20 | Gen Loss: 1.9077, Disc Loss: 0.3981\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_6_step_6.png\n",
      "Epoch 6/20 | Gen Loss: 3.2542, Disc Loss: 0.2513\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_7_step_6.png\n",
      "Epoch 7/20 | Gen Loss: 4.0303, Disc Loss: 0.1168\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_8_step_6.png\n",
      "Epoch 8/20 | Gen Loss: 3.9870, Disc Loss: 0.0963\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_9_step_6.png\n",
      "Epoch 9/20 | Gen Loss: 3.4657, Disc Loss: 0.1318\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_10_step_6.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Gen Loss: 3.9407, Disc Loss: 0.2680\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_11_step_6.png\n",
      "Epoch 11/20 | Gen Loss: 4.1008, Disc Loss: 0.0510\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_12_step_6.png\n",
      "Epoch 12/20 | Gen Loss: 4.1702, Disc Loss: 0.0639\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_13_step_6.png\n",
      "Epoch 13/20 | Gen Loss: 4.1942, Disc Loss: 0.0638\n",
      "Visualization saved and logged: ./training_logs\\test_results_epoch_14_step_6.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m disc_loss_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_images, gt_images \u001b[38;5;129;01min\u001b[39;00m train_data:  \u001b[38;5;66;03m# Iterate over training batches\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     gen_loss, disc_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     gen_loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m gen_loss\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     21\u001b[0m     disc_loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m disc_loss\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 60\n",
    "log_path = \"./training_logs\"\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"epochs\", EPOCHS)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"image_size\", (200, 200))\n",
    "    mlflow.log_param(\"batch_size\", 32)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        step = 0\n",
    "        gen_loss_epoch = 0\n",
    "        disc_loss_epoch = 0\n",
    "        \n",
    "        for input_images, gt_images in train_data:  # Iterate over training batches\n",
    "            gen_loss, disc_loss = train_step(generator, discriminator, input_images, gt_images, gen_optimizer, disc_optimizer)\n",
    "            \n",
    "            gen_loss_epoch += gen_loss.numpy()\n",
    "            disc_loss_epoch += disc_loss.numpy()\n",
    "            step += 1\n",
    "\n",
    "            # Visualize and log images every 100 steps\n",
    "            if step % 5 == 0:\n",
    "                visualize_and_log_images(generator, input_images, gt_images, log_path, epoch, step)\n",
    "\n",
    "        # Average losses\n",
    "        gen_loss_epoch /= step\n",
    "        disc_loss_epoch /= step\n",
    "\n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metric(\"generator_loss\", gen_loss_epoch, step=epoch)\n",
    "        mlflow.log_metric(\"discriminator_loss\", disc_loss_epoch, step=epoch)\n",
    "\n",
    "        # Save models every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            gen_model_path = f\"generator_epoch_{epoch+1}.h5\"\n",
    "            disc_model_path = f\"discriminator_epoch_{epoch+1}.h5\"\n",
    "            generator.save(gen_model_path)\n",
    "            discriminator.save(disc_model_path)\n",
    "            mlflow.log_artifact(gen_model_path, artifact_path=\"models\")\n",
    "            mlflow.log_artifact(disc_model_path, artifact_path=\"models\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Gen Loss: {gen_loss_epoch:.4f}, Disc Loss: {disc_loss_epoch:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved and logged: ./test_visualizations\\test_batch_1.png\n",
      "Visualization saved and logged: ./test_visualizations\\test_batch_2.png\n",
      "Visualization saved and logged: ./test_visualizations\\test_batch_3.png\n",
      "Visualization saved and logged: ./test_visualizations\\test_batch_4.png\n",
      "Test Gen Loss: 1.4358, Test Disc Loss: 0.3656\n"
     ]
    }
   ],
   "source": [
    "test_gen_loss = 0\n",
    "test_disc_loss = 0\n",
    "test_steps = 0\n",
    "\n",
    "log_path = \"./test_visualizations\"\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "for i, (test_input_images, test_gt_images) in enumerate(test_data):\n",
    "    generated_images = generator(test_input_images, training=False)\n",
    "    fake_output = discriminator(generated_images, training=False)\n",
    "    real_output = discriminator(test_gt_images, training=False)\n",
    "\n",
    "    test_gen_loss += gan_loss(test_gt_images, generated_images, fake_output).numpy()\n",
    "    test_disc_loss += (\n",
    "        tf.keras.losses.BinaryCrossentropy()(tf.ones_like(real_output), real_output).numpy() +\n",
    "        tf.keras.losses.BinaryCrossentropy()(tf.zeros_like(fake_output), fake_output).numpy()\n",
    "    )\n",
    "    test_steps += 1\n",
    "\n",
    "    # Visualize and log test images (limit to 5 batches for clarity)\n",
    "    if i < 5:\n",
    "        visualize_and_log_images(\n",
    "            generator, test_input_images, test_gt_images, log_path, prefix=f\"test_batch_{i+1}\"\n",
    "        )\n",
    "\n",
    "test_gen_loss /= test_steps\n",
    "test_disc_loss /= test_steps\n",
    "\n",
    "print(f\"Test Gen Loss: {test_gen_loss:.4f}, Test Disc Loss: {test_disc_loss:.4f}\")\n",
    "mlflow.log_metric(\"test_generator_loss\", test_gen_loss)\n",
    "mlflow.log_metric(\"test_discriminator_loss\", test_disc_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
